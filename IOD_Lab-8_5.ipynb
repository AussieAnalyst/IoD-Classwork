{"cells":[{"cell_type":"markdown","metadata":{"id":"Nji1a9ULLtCA"},"source":["<div>\n","<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"fnsX1AWKLtCE"},"source":["# Lab 8.5: Text Classification\n","INSTRUCTIONS:\n","- Run the cells\n","- Observe and understand the results\n","- Answer the questions"]},{"cell_type":"markdown","metadata":{"id":"6pm8PttyLtCI"},"source":["## Import libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:38:33.182995Z","start_time":"2019-06-17T01:38:30.045388Z"},"id":"EUANiH6zLtCK"},"outputs":[],"source":["## Import Libraries\n","import numpy as np\n","import pandas as pd\n","\n","import string\n","import spacy\n","\n","from collections import Counter\n","\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC\n","\n","# import warnings\n","# warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"58bUNQA0LtCV"},"source":["## Load data"]},{"cell_type":"markdown","metadata":{"id":"UqU7d_qcLtCX"},"source":["Sample:\n","\n","    __label__2 Stuning even for the non-gamer: This sound ...\n","    __label__2 The best soundtrack ever to anything.: I'm ...\n","    __label__2 Amazing!: This soundtrack is my favorite m ...\n","    __label__2 Excellent Soundtrack: I truly like this so ...\n","    __label__2 Remember, Pull Your Jaw Off The Floor Afte ...\n","    __label__2 an absolute masterpiece: I am quite sure a ...\n","    __label__1 Buyer beware: This is a self-published boo ...\n","    . . .\n","    \n","There are only two **labels**:\n","- `__label__1`\n","- `__label__2`"]},{"cell_type":"code","execution_count":37,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:38:42.024845Z","start_time":"2019-06-17T01:38:41.896098Z"},"id":"rwWFJprZLtCZ"},"outputs":[],"source":["## Loading the data\n","\n","trainDF = pd.read_fwf(\n","    filepath_or_buffer = 'data/corpus.txt',\n","    colspecs = [(9, 10),   # label: get only the numbers 1 or 2\n","                (11, 9000) # text: makes the it big enought to get to the end of the line\n","               ], \n","    header = 0,\n","    names = ['label', 'text'],\n","    lineterminator = '\\n'\n",")\n","\n","# convert label from [1, 2] to [0, 1]\n","trainDF['label'] = trainDF['label'] - 1"]},{"cell_type":"markdown","metadata":{"id":"mILVIHomLtCf"},"source":["## Inspect the data"]},{"cell_type":"code","execution_count":38,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:39:24.213192Z","start_time":"2019-06-17T01:39:24.209202Z"},"id":"G9_8RbOeLtCh"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9427</th>\n","      <td>1</td>\n","      <td>camco wrench: This is a great tool. Due to art...</td>\n","    </tr>\n","    <tr>\n","      <th>174</th>\n","      <td>1</td>\n","      <td>California Exotics Waterproof Delights Blue Ba...</td>\n","    </tr>\n","    <tr>\n","      <th>2255</th>\n","      <td>0</td>\n","      <td>Why Can't I Rate at Zero Stars ?: Come On, Peo...</td>\n","    </tr>\n","    <tr>\n","      <th>7371</th>\n","      <td>0</td>\n","      <td>Expectations: Not being a passionate Science F...</td>\n","    </tr>\n","    <tr>\n","      <th>3773</th>\n","      <td>1</td>\n","      <td>Very good heart wrenching story: I read this b...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      label                                               text\n","9427      1  camco wrench: This is a great tool. Due to art...\n","174       1  California Exotics Waterproof Delights Blue Ba...\n","2255      0  Why Can't I Rate at Zero Stars ?: Come On, Peo...\n","7371      0  Expectations: Not being a passionate Science F...\n","3773      1  Very good heart wrenching story: I read this b..."]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["# ANSWER\n","trainDF.sample(5)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/plain":["'Glorious story: I loved Whisper of the wicked saints. The story was amazing and I was pleasantly surprised at the changes in the book. I am not normaly someone who is into romance novels, but the world was raving about this book and so I bought it. I loved it !! This is a brilliant story because it is so true. This book was so wonderful that I have told all of my friends to read it. It is not a typical romance, it is so much more. Not reading this book is a crime, becuase you are missing out on a heart warming story.'"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["trainDF['text'].iloc[6]"]},{"cell_type":"markdown","metadata":{"id":"6YmYgG2pLtCu"},"source":["## Split the data into train and test"]},{"cell_type":"code","execution_count":40,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:39:40.103737Z","start_time":"2019-06-17T01:39:40.100739Z"},"id":"j5vErjWFLtCy"},"outputs":[],"source":["## ANSWER\n","## split the dataset\n","X = trainDF['text']\n","y = trainDF['label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)"]},{"cell_type":"markdown","metadata":{"id":"6nUp6oDOLtC1"},"source":["## Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"fKd9yTnyLtC2"},"source":["### Count Vectors as features"]},{"cell_type":"code","execution_count":41,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:40:32.674674Z","start_time":"2019-06-17T01:40:31.098889Z"},"id":"DU2RqqDjLtC3"},"outputs":[],"source":["# create a count vectorizer object\n","count_vect = CountVectorizer(token_pattern = r'\\w{1,}')\n","\n","# Learn a vocabulary dictionary of all tokens in the raw documents\n","count_vect.fit(trainDF['text'])\n","\n","# Transform documents to document-term matrix.\n","X_train_count = count_vect.transform(X_train)\n","X_test_count = count_vect.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"dJs6al0ILtC5"},"source":["### TF-IDF Vectors as features\n","- Word level\n","- N-Gram level\n","- Character level"]},{"cell_type":"code","execution_count":42,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:40:36.088730Z","start_time":"2019-06-17T01:40:34.519925Z"},"id":"myjfdfP_LtC6","outputId":"8bc4d529-1f66-4836-acd1-07633c29fd02"},"outputs":[{"name":"stdout","output_type":"stream","text":["TfidfVectorizer(max_features=5000, token_pattern='\\\\w{1,}')\n","CPU times: total: 1.06 s\n","Wall time: 1.16 s\n"]}],"source":["%%time\n","# word level tf-idf\n","tfidf_vect = TfidfVectorizer(analyzer = 'word',\n","                             token_pattern = r'\\w{1,}',\n","                             max_features = 5000)\n","print(tfidf_vect)\n","\n","tfidf_vect.fit(trainDF['text'])\n","X_train_tfidf = tfidf_vect.transform(X_train)\n","X_test_tfidf  = tfidf_vect.transform(X_test)"]},{"cell_type":"code","execution_count":43,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:40:57.505221Z","start_time":"2019-06-17T01:40:49.387393Z"},"id":"-h16dUaVLtC_","outputId":"3be1f8f5-670e-4249-8522-50509c8898a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["TfidfVectorizer(max_features=5000, ngram_range=(2, 3), token_pattern='\\\\w{1,}')\n","CPU times: total: 5.25 s\n","Wall time: 5.44 s\n"]}],"source":["%%time\n","# ngram level tf-idf\n","tfidf_vect_ngram = TfidfVectorizer(analyzer = 'word',\n","                                   token_pattern = r'\\w{1,}',\n","                                   ngram_range = (2, 3),\n","                                   max_features = 5000)\n","print(tfidf_vect_ngram)\n","\n","tfidf_vect_ngram.fit(trainDF['text'])\n","X_train_tfidf_ngram = tfidf_vect_ngram.transform(X_train)\n","X_test_tfidf_ngram  = tfidf_vect_ngram.transform(X_test)"]},{"cell_type":"code","execution_count":44,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:41:10.209071Z","start_time":"2019-06-17T01:40:59.211484Z"},"id":"Y7rmIt49LtDC","outputId":"8f600e7c-b4df-4d89-bfb9-96c8436aa5ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["TfidfVectorizer(analyzer='char', max_features=5000, ngram_range=(2, 3),\n","                token_pattern='\\\\w{1,}')\n"]},{"name":"stderr","output_type":"stream","text":["C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:546: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: total: 7.86 s\n","Wall time: 8.03 s\n"]}],"source":["%%time\n","# characters level tf-idf\n","tfidf_vect_ngram_chars = TfidfVectorizer(analyzer = 'char',\n","                                         token_pattern = r'\\w{1,}',\n","                                         ngram_range = (2, 3),\n","                                         max_features = 5000)\n","print(tfidf_vect_ngram_chars)\n","\n","tfidf_vect_ngram_chars.fit(trainDF['text'])\n","X_train_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_train)\n","X_test_tfidf_ngram_chars  = tfidf_vect_ngram_chars.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"_Pck1cuvLtDH"},"source":["### Text / NLP based features\n","\n","Create some other features.\n","\n","Char_Count = Number of Characters in Text\n","\n","Word Count = Number of Words in Text\n","\n","Word Density = Average Number of Char in Words\n","\n","Punctuation Count = Number of Punctuation in Text\n","\n","Title Word Count = Number of Words in Title\n","\n","Uppercase Word Count = Number of Upperwords in Text"]},{"cell_type":"code","execution_count":45,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:43:52.813132Z","start_time":"2019-06-17T01:43:52.806150Z"},"id":"4jebGm1gLtDH","outputId":"124d1ac4-9b64-414d-e973-9ded78662e18"},"outputs":[],"source":["# ANSWER\n","import re\n","def create_features(text):\n","    # character count\n","    Char_Count = len(text)\n","\n","    # word count\n","    Word_Count = len(text.strip().split(' '))\n","\n","    # word density\n","    text_no_punct = re.sub(r'[/\"\\.,!\\?\\']', '', text)\n","    Word_Density = np.mean([len(word) for word in text_no_punct.strip().split(' ')])\n","\n","    # punctuation count\n","    text_only_punct = re.sub(r'[^/\"\\.,!?\\']', '', text)\n","    Punct_Count = len(text_only_punct)\n","\n","    # title word count\n","    title = re.findall(r'^.*:', text)\n","    Title_Word_Count = len(str(title).strip().split(' '))\n","\n","    # uppercase word count\n","    upper = re.findall(r'\\b[A-Z]\\w*\\b', text)\n","    Upper_Word_Count = len(upper)\n","\n","    # return all into a list\n","    return (Char_Count, Word_Count, Word_Density, Punct_Count, Title_Word_Count, Upper_Word_Count)\n","\n"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 1.3 s\n","Wall time: 1.32 s\n"]}],"source":["%%time\n","# Apply to the data and add to features\n","features_list = []\n","for i in trainDF.index:\n","    features = create_features(trainDF['text'].iloc[i])\n","    features_list.append(features)\n","\n","# Add to dataframe\n","cols=['Char_Count', 'Word_Count', 'Word_Density', 'Punct_Count', 'Title_Word_Count', 'Upper_Word_Count']\n","trainDF[cols] = features_list"]},{"cell_type":"code","execution_count":47,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:44:03.442730Z","start_time":"2019-06-17T01:44:02.298791Z"},"id":"Z-l2iZcLLtDO","scrolled":false},"outputs":[],"source":["## load spaCy\n","nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"markdown","metadata":{"id":"p-9d0G59LtDR"},"source":["Part of Speech in **SpaCy**\n","\n","    POS   DESCRIPTION               EXAMPLES\n","    ----- ------------------------- ---------------------------------------------\n","    ADJ   adjective                 big, old, green, incomprehensible, first\n","    ADP   adposition                in, to, during\n","    ADV   adverb                    very, tomorrow, down, where, there\n","    AUX   auxiliary                 is, has (done), will (do), should (do)\n","    CONJ  conjunction               and, or, but\n","    CCONJ coordinating conjunction  and, or, but\n","    DET   determiner                a, an, the\n","    INTJ  interjection              psst, ouch, bravo, hello\n","    NOUN  noun                      girl, cat, tree, air, beauty\n","    NUM   numeral                   1, 2017, one, seventy-seven, IV, MMXIV\n","    PART  particle                  's, not,\n","    PRON  pronoun                   I, you, he, she, myself, themselves, somebody\n","    PROPN proper noun               Mary, John, London, NATO, HBO\n","    PUNCT punctuation               ., (, ), ?\n","    SCONJ subordinating conjunction if, while, that\n","    SYM   symbol                    $, %, §, ©, +, −, ×, ÷, =, :), 😝\n","    VERB  verb                      run, runs, running, eat, ate, eating\n","    X     other                     sfpksdpsxmsa\n","    SPACE space\n","    \n","Find out number of Adjective, Adverb, Noun, Numeric, Pronoun, Proposition, Verb.\n","\n","    Hint:\n","    1. Convert text to spacy document\n","    2. Use pos_\n","    3. Use Counter "]},{"cell_type":"code","execution_count":48,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:50:15.900377Z","start_time":"2019-06-17T01:50:15.889406Z"},"id":"NcxmvIOGLtDS"},"outputs":[],"source":["# Initialise some columns for feature's counts\n","trainDF['adj_count'] = 0\n","trainDF['adv_count'] = 0\n","trainDF['noun_count'] = 0\n","trainDF['num_count'] = 0\n","trainDF['pron_count'] = 0\n","trainDF['propn_count'] = 0\n","trainDF['verb_count'] = 0"]},{"cell_type":"code","execution_count":49,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:52:39.611809Z","start_time":"2019-06-17T01:52:39.608818Z"},"id":"1KfFtu1HcPwA"},"outputs":[],"source":["# ANSWER\n","def POS_Values(text):\n","    # Tokenize\n","    doc = nlp(text)\n","\n","    # Get POS counts\n","    # adjective\n","    adj_count = len([word for word in doc if word.pos_ == 'ADJ'])\n","    # adverb\n","    adv_count = len([word for word in doc if word.pos_ == 'ADV'])\n","    # noun\n","    noun_count = len([word for word in doc if word.pos_ == 'NOUN'])\n","    # numerals\n","    num_count = len([word for word in doc if word.pos_ == 'NUM'])\n","    # pronoun\n","    pron_count = len([word for word in doc if word.pos_ == 'PRON'])\n","    # proper noun\n","    propn_count = len([word for word in doc if word.pos_ == 'PROPN'])\n","    # verb\n","    verb_count = len([word for word in doc if word.pos_ == 'VERB'])\n","\n","    # return all as tuple\n","    return (adj_count, adv_count, noun_count, num_count, pron_count, propn_count, verb_count)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 2min 22s\n","Wall time: 2min 35s\n"]}],"source":["%%time\n","# Apply to the data and add to features\n","spacy_feat_list = []\n","for text in trainDF['text']:\n","    features = POS_Values(text)\n","    spacy_feat_list.append(features)\n","\n","# Add to dataframe\n","cols=['adj_count',\n","    'adv_count', 'noun_count', 'num_count',\n","    'pron_count', 'propn_count', 'verb_count']\n","trainDF[cols] = spacy_feat_list"]},{"cell_type":"code","execution_count":52,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:59:42.424828Z","start_time":"2019-06-17T01:59:42.390920Z"},"id":"DW1_LKP2LtDX","outputId":"7a5eb5fd-cae1-4e76-f95f-e0fe9f1780d8","scrolled":false},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Char_Count</th>\n","      <th>Word_Count</th>\n","      <th>Word_Density</th>\n","      <th>Punct_Count</th>\n","      <th>Title_Word_Count</th>\n","      <th>Upper_Word_Count</th>\n","      <th>adj_count</th>\n","      <th>adv_count</th>\n","      <th>noun_count</th>\n","      <th>num_count</th>\n","      <th>pron_count</th>\n","      <th>propn_count</th>\n","      <th>verb_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6441</th>\n","      <td>666.0</td>\n","      <td>109.0</td>\n","      <td>4.972477</td>\n","      <td>16.0</td>\n","      <td>3.0</td>\n","      <td>8.0</td>\n","      <td>14</td>\n","      <td>8</td>\n","      <td>23</td>\n","      <td>1</td>\n","      <td>12</td>\n","      <td>1</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>8390</th>\n","      <td>145.0</td>\n","      <td>25.0</td>\n","      <td>4.400000</td>\n","      <td>11.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7003</th>\n","      <td>999.0</td>\n","      <td>184.0</td>\n","      <td>4.342391</td>\n","      <td>17.0</td>\n","      <td>1.0</td>\n","      <td>23.0</td>\n","      <td>15</td>\n","      <td>12</td>\n","      <td>28</td>\n","      <td>6</td>\n","      <td>21</td>\n","      <td>6</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>1946</th>\n","      <td>496.0</td>\n","      <td>89.0</td>\n","      <td>4.494382</td>\n","      <td>8.0</td>\n","      <td>16.0</td>\n","      <td>11.0</td>\n","      <td>9</td>\n","      <td>3</td>\n","      <td>18</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>5</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>7188</th>\n","      <td>600.0</td>\n","      <td>111.0</td>\n","      <td>4.225225</td>\n","      <td>21.0</td>\n","      <td>5.0</td>\n","      <td>12.0</td>\n","      <td>8</td>\n","      <td>4</td>\n","      <td>23</td>\n","      <td>3</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>12</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Char_Count  Word_Count  Word_Density  Punct_Count  Title_Word_Count  \\\n","6441       666.0       109.0      4.972477         16.0               3.0   \n","8390       145.0        25.0      4.400000         11.0               1.0   \n","7003       999.0       184.0      4.342391         17.0               1.0   \n","1946       496.0        89.0      4.494382          8.0              16.0   \n","7188       600.0       111.0      4.225225         21.0               5.0   \n","\n","      Upper_Word_Count  adj_count  adv_count  noun_count  num_count  \\\n","6441               8.0         14          8          23          1   \n","8390               7.0          4          1           6          0   \n","7003              23.0         15         12          28          6   \n","1946              11.0          9          3          18          0   \n","7188              12.0          8          4          23          3   \n","\n","      pron_count  propn_count  verb_count  \n","6441          12            1          12  \n","8390           0            1           1  \n","7003          21            6          25  \n","1946           8            5           9  \n","7188          10            5          12  "]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["cols = [\n","    'Char_Count', 'Word_Count', 'Word_Density', \n","    'Punct_Count', 'Title_Word_Count', 'Upper_Word_Count', \n","    'adj_count', 'adv_count', 'noun_count', 'num_count',\n","    'pron_count', 'propn_count', 'verb_count']\n","\n","trainDF[cols].sample(5)"]},{"cell_type":"markdown","metadata":{"id":"mQCAUFWYLtDb"},"source":["### Topic Models as features"]},{"cell_type":"code","execution_count":53,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:27:09.442903Z","start_time":"2019-06-17T02:24:45.531924Z"},"id":"wg2mAlkRLtDb","outputId":"323bfce4-9263-403a-9214-e9e206d5755f"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 1min 2s\n","Wall time: 1min 6s\n"]},{"name":"stderr","output_type":"stream","text":["C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}],"source":["%%time\n","# train a LDA Model\n","lda_model = LatentDirichletAllocation(n_components = 20, learning_method = 'online', max_iter = 20)\n","\n","X_topics = lda_model.fit_transform(X_train_count)\n","topic_word = lda_model.components_ \n","vocab = count_vect.get_feature_names()"]},{"cell_type":"code","execution_count":54,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:28:11.804475Z","start_time":"2019-06-17T02:28:10.978502Z"},"id":"_8dIDyHjLtDf","outputId":"ea0614e2-66f1-4ddd-bff0-142cfd4fd78a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Group Top Words\n","----- --------------------------------------------------------------------------------\n","    0 the book of and a is in read to this\n","    1 game card games oh graphics play box memory software computer\n","    2 mars sign 1985 enlightenment relation accepted regain ipad nichols cheer\n","    3 1984 orwell 3d george david winston brother effective hockey version\n","    4 ugly rabbit coyote bar perform york disney beads string slot\n","    5 la de en y con el que catherine del es\n","    6 jawbone metal eargels hook unusual inspiring textbook tears hd packaged\n","    7 ear fit rubber jabra economic headset squeem commercial minor wasnt\n","    8 cents flea fleas crossed hendrix organization scam edition noni bela\n","    9 of cd is music the album s songs and song\n","   10 hands phone vampire hostel cars awsome bug occasional calling isnt\n","   11 camera battery product works use the charger charge computer fire\n","   12 broke boot hot heat following heater socks describe brando mountain\n","   13 batteries 24 charged kidding bd spider fitness import 4 26\n","   14 the i it and a to this is of for\n","   15 command platform brio positions neil pulley timothy molly dwarf unfocused\n","   16 attic doll bdsm regarded delights huppert clubs russia dolls texts\n","   17 tool 99 screw communism rollerball mindless gabriel unbearable disease sales\n","   18 air bed van damme inflated et relationships beds sample un\n","   19 blah wanting religious promised ho keyboard fish mike fantasies chasing\n"]}],"source":["# view the topic models\n","n_top_words = 10\n","topic_summaries = []\n","print('Group Top Words')\n","print('-----', '-'*80)\n","for i, topic_dist in enumerate(topic_word):\n","    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n","    top_words = ' '.join(topic_words)\n","    topic_summaries.append(top_words)\n","    print('  %3d %s' % (i, top_words))"]},{"cell_type":"markdown","metadata":{"id":"TtfnK1jeLtDl"},"source":["## Modelling"]},{"cell_type":"code","execution_count":55,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:12.273365Z","start_time":"2019-06-17T02:34:12.263393Z"},"id":"uwVaWSyTLtDm"},"outputs":[],"source":["## helper function\n","\n","def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n","    # fit the training dataset on the classifier\n","    classifier.fit(feature_vector_train, label)\n","\n","    # predict the labels on validation dataset\n","    predictions = classifier.predict(feature_vector_valid)\n","\n","    return accuracy_score(predictions, y_test)"]},{"cell_type":"code","execution_count":56,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:14.900001Z","start_time":"2019-06-17T02:34:14.894016Z"},"id":"f_onpqUkLtDo"},"outputs":[],"source":["# Keep the results in a dataframe\n","results = pd.DataFrame(columns = ['Count Vectors',\n","                                  'WordLevel TF-IDF',\n","                                  'N-Gram Vectors',\n","                                  'CharLevel Vectors'])"]},{"cell_type":"markdown","metadata":{"id":"OXwLriDpLtDq"},"source":["### Naive Bayes Classifier"]},{"cell_type":"code","execution_count":57,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:34.147043Z","start_time":"2019-06-17T02:34:34.123096Z"},"id":"ZcU6IKyNLtDs","outputId":"d2defcfb-2046-47e1-b3b6-08d6edca94f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["NB, Count Vectors    : 0.8465\n","\n","CPU times: total: 0 ns\n","Wall time: 7.98 ms\n"]}],"source":["%%time\n","# Naive Bayes on Count Vectors\n","accuracy1 = train_model(MultinomialNB(), X_train_count, y_train, X_test_count)\n","print('NB, Count Vectors    : %.4f\\n' % accuracy1)"]},{"cell_type":"code","execution_count":58,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:36.399812Z","start_time":"2019-06-17T02:34:36.381861Z"},"id":"zqEG_ByTLtDv","outputId":"0ba74f49-a71c-4d59-f57b-f39f546241ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["NB, WordLevel TF-IDF : 0.8550\n","\n","CPU times: total: 31.2 ms\n","Wall time: 6.98 ms\n"]}],"source":["%%time\n","# Naive Bayes on Word Level TF IDF Vectors\n","accuracy2 = train_model(MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf)\n","print('NB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"]},{"cell_type":"code","execution_count":59,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:39.076000Z","start_time":"2019-06-17T02:34:39.059047Z"},"id":"uKEEPNi8LtDy","outputId":"c00cf532-c914-4670-cde5-69bf54bf7201"},"outputs":[{"name":"stdout","output_type":"stream","text":["NB, N-Gram Vectors   : 0.8370\n","\n","CPU times: total: 0 ns\n","Wall time: 4.99 ms\n"]}],"source":["%%time\n","# Naive Bayes on Ngram Level TF IDF Vectors\n","accuracy3 = train_model(MultinomialNB(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n","print('NB, N-Gram Vectors   : %.4f\\n' % accuracy3)"]},{"cell_type":"code","execution_count":60,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:42.057019Z","start_time":"2019-06-17T02:34:42.009151Z"},"id":"M9aIibkBLtD0","outputId":"1f52c1f4-150d-4195-f927-cb66ec41baba"},"outputs":[{"name":"stdout","output_type":"stream","text":["NB, CharLevel Vectors: 0.8060\n","\n","CPU times: total: 15.6 ms\n","Wall time: 25.9 ms\n"]}],"source":["%%time\n","# # Naive Bayes on Character Level TF IDF Vectors\n","accuracy4 = train_model(MultinomialNB(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n","print('NB, CharLevel Vectors: %.4f\\n' % accuracy4)"]},{"cell_type":"code","execution_count":61,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:46.265712Z","start_time":"2019-06-17T02:34:46.258734Z"},"id":"kkrodUzCLtD3"},"outputs":[],"source":["results.loc['Naïve Bayes'] = {\n","    'Count Vectors': accuracy1,\n","    'WordLevel TF-IDF': accuracy2,\n","    'N-Gram Vectors': accuracy3,\n","    'CharLevel Vectors': accuracy4}"]},{"cell_type":"markdown","metadata":{"id":"-2oNfajULtD4"},"source":["### Linear Classifier"]},{"cell_type":"code","execution_count":62,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:50.841687Z","start_time":"2019-06-17T02:34:48.637032Z"},"id":"OFBhhPZ6LtD4","outputId":"cb5a2b19-dfc1-4b11-e698-0488ba4eae53"},"outputs":[{"name":"stdout","output_type":"stream","text":["LR, Count Vectors    : 0.8730\n","\n","CPU times: total: 1.66 s\n","Wall time: 1.91 s\n"]}],"source":["%%time\n","# Linear Classifier on Count Vectors\n","accuracy1 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 350), X_train_count, y_train, X_test_count)\n","print('LR, Count Vectors    : %.4f\\n' % accuracy1)"]},{"cell_type":"code","execution_count":63,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:51.310433Z","start_time":"2019-06-17T02:34:51.214690Z"},"id":"C89hRhDiLtD6","outputId":"023669b1-788f-4a1b-d282-99464e26312c"},"outputs":[{"name":"stdout","output_type":"stream","text":["LR, WordLevel TF-IDF : 0.8780\n","\n","CPU times: total: 78.1 ms\n","Wall time: 77.8 ms\n"]}],"source":["%%time\n","# Linear Classifier on Word Level TF IDF Vectors\n","accuracy2 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf, y_train, X_test_tfidf)\n","print('LR, WordLevel TF-IDF : %.4f\\n' % accuracy2)"]},{"cell_type":"code","execution_count":64,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:51.749258Z","start_time":"2019-06-17T02:34:51.683435Z"},"id":"MvhV1jC6LtD9","outputId":"9bef9b1d-2dc3-4300-c4ff-831957aadee4"},"outputs":[{"name":"stdout","output_type":"stream","text":["LR, N-Gram Vectors   : 0.8435\n","\n","CPU times: total: 46.9 ms\n","Wall time: 46.9 ms\n"]}],"source":["%%time\n","# Linear Classifier on Ngram Level TF IDF Vectors\n","accuracy3 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n","print('LR, N-Gram Vectors   : %.4f\\n' % accuracy3)"]},{"cell_type":"code","execution_count":65,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:52.635899Z","start_time":"2019-06-17T02:34:52.175122Z"},"id":"XPjIxmtKLtEA","outputId":"58f4b0e9-e786-45a1-830e-5945129792d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["LR, CharLevel Vectors: 0.8445\n","\n","CPU times: total: 281 ms\n","Wall time: 300 ms\n"]}],"source":["%%time\n","# Linear Classifier on Character Level TF IDF Vectors\n","accuracy4 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n","print('LR, CharLevel Vectors: %.4f\\n' % accuracy4)"]},{"cell_type":"code","execution_count":66,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:53.029844Z","start_time":"2019-06-17T02:34:53.018872Z"},"id":"ZFK_LWTcLtED"},"outputs":[],"source":["results.loc['Logistic Regression'] = {\n","    'Count Vectors': accuracy1,\n","    'WordLevel TF-IDF': accuracy2,\n","    'N-Gram Vectors': accuracy3,\n","    'CharLevel Vectors': accuracy4}"]},{"cell_type":"markdown","metadata":{"id":"q1wYto68LtEE"},"source":["### Support Vector Machine"]},{"cell_type":"code","execution_count":67,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:54.237613Z","start_time":"2019-06-17T02:34:53.406835Z"},"id":"yYGz8he5LtEE","outputId":"546b1ea8-27c9-45bb-fd91-da08244d6796"},"outputs":[{"name":"stdout","output_type":"stream","text":["SVM, Count Vectors    : 0.8465\n","\n","CPU times: total: 547 ms\n","Wall time: 563 ms\n"]}],"source":["%%time\n","# Support Vector Machine on Count Vectors\n","accuracy1 = train_model(LinearSVC(), X_train_count, y_train, X_test_count)\n","print('SVM, Count Vectors    : %.4f\\n' % accuracy1)"]},{"cell_type":"code","execution_count":68,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:54.743263Z","start_time":"2019-06-17T02:34:54.606629Z"},"id":"wMt26K0cLtEG","outputId":"406cf62d-c09d-4f9b-c298-eabce0b69238"},"outputs":[{"name":"stdout","output_type":"stream","text":["SVM, WordLevel TF-IDF : 0.8710\n","\n","CPU times: total: 93.8 ms\n","Wall time: 68.8 ms\n"]}],"source":["%%time\n","# Support Vector Machine on Word Level TF IDF Vectors\n","accuracy2 = train_model(LinearSVC(), X_train_tfidf, y_train, X_test_tfidf)\n","print('SVM, WordLevel TF-IDF : %.4f\\n' % accuracy2)"]},{"cell_type":"code","execution_count":69,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:55.220003Z","start_time":"2019-06-17T02:34:55.119256Z"},"id":"eFt6Y1VvLtEI","outputId":"628299db-8bca-4698-c64e-5b291bd248e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["SVM, N-Gram Vectors   : 0.8330\n","\n","CPU times: total: 46.9 ms\n","Wall time: 53.9 ms\n"]}],"source":["%%time\n","# Support Vector Machine on Ngram Level TF IDF Vectors\n","accuracy3 = train_model(LinearSVC(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n","print('SVM, N-Gram Vectors   : %.4f\\n' % accuracy3)"]},{"cell_type":"code","execution_count":70,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:56.139528Z","start_time":"2019-06-17T02:34:55.585010Z"},"id":"iqhsS579LtEL","outputId":"7cc3d723-9858-43a8-dcdf-37fcbef9456e"},"outputs":[{"name":"stdout","output_type":"stream","text":["SVM, CharLevel Vectors: 0.8545\n","\n","CPU times: total: 359 ms\n","Wall time: 376 ms\n"]}],"source":["%%time\n","# Support Vector Machine on Character Level TF IDF Vectors\n","accuracy4 = train_model(LinearSVC(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n","print('SVM, CharLevel Vectors: %.4f\\n' % accuracy4)"]},{"cell_type":"code","execution_count":71,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:34:56.501558Z","start_time":"2019-06-17T02:34:56.492592Z"},"id":"go0bcKeILtEN"},"outputs":[],"source":["results.loc['Support Vector Machine'] = {\n","    'Count Vectors': accuracy1,\n","    'WordLevel TF-IDF': accuracy2,\n","    'N-Gram Vectors': accuracy3,\n","    'CharLevel Vectors': accuracy4}"]},{"cell_type":"markdown","metadata":{"id":"gLGxWK0yLtEO"},"source":["### Bagging Models"]},{"cell_type":"code","execution_count":72,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:35:20.807157Z","start_time":"2019-06-17T02:34:56.823697Z"},"id":"HR8aOytWLtEO","outputId":"961d5bca-c1fe-46be-ba8c-2f6325d85901"},"outputs":[{"name":"stdout","output_type":"stream","text":["RF, Count Vectors    : 0.8300\n","\n","CPU times: total: 12.4 s\n","Wall time: 12.7 s\n"]}],"source":["%%time\n","# Bagging (Random Forest) on Count Vectors\n","accuracy1 = train_model(RandomForestClassifier(n_estimators = 100), X_train_count, y_train, X_test_count)\n","print('RF, Count Vectors    : %.4f\\n' % accuracy1)"]},{"cell_type":"code","execution_count":73,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:35:30.127233Z","start_time":"2019-06-17T02:35:21.198110Z"},"id":"zXcTCTEGLtET","outputId":"e4336045-c508-4372-e102-35c559f9a195"},"outputs":[{"name":"stdout","output_type":"stream","text":["RF, WordLevel TF-IDF : 0.8385\n","\n","CPU times: total: 5.56 s\n","Wall time: 5.91 s\n"]}],"source":["%%time\n","# Bagging (Random Forest) on Word Level TF IDF Vectors\n","accuracy2 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf, y_train, X_test_tfidf)\n","print('RF, WordLevel TF-IDF : %.4f\\n' % accuracy2)"]},{"cell_type":"code","execution_count":74,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:35:40.162390Z","start_time":"2019-06-17T02:35:30.607944Z"},"id":"EnvT8qvSLtEW","outputId":"0c3a8fa0-7ee7-40ad-dc0e-85a8c3995733"},"outputs":[{"name":"stdout","output_type":"stream","text":["RF, N-Gram Vectors   : 0.7945\n","\n","CPU times: total: 5.11 s\n","Wall time: 5.56 s\n"]}],"source":["%%time\n","# Bagging (Random Forest) on Ngram Level TF IDF Vectors\n","accuracy3 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n","print('RF, N-Gram Vectors   : %.4f\\n' % accuracy3)"]},{"cell_type":"code","execution_count":75,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:36:07.640904Z","start_time":"2019-06-17T02:35:40.542371Z"},"id":"8jt-dTVELtEX","outputId":"dc1113c9-b3f2-4d2e-f4d7-a5bcc98a020c"},"outputs":[{"name":"stdout","output_type":"stream","text":["RF, CharLevel Vectors: 0.7960\n","\n","CPU times: total: 19.1 s\n","Wall time: 21.6 s\n"]}],"source":["%%time\n","# Bagging (Random Forest) on Character Level TF IDF Vectors\n","accuracy4 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n","print('RF, CharLevel Vectors: %.4f\\n' % accuracy4)"]},{"cell_type":"code","execution_count":76,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:36:08.104108Z","start_time":"2019-06-17T02:36:08.097127Z"},"id":"fVKeCH_VLtEZ"},"outputs":[],"source":["results.loc['Random Forest'] = {\n","    'Count Vectors': accuracy1,\n","    'WordLevel TF-IDF': accuracy2,\n","    'N-Gram Vectors': accuracy3,\n","    'CharLevel Vectors': accuracy4}"]},{"cell_type":"markdown","metadata":{"id":"oyVz4Q6ILtEa"},"source":["### Boosting Models"]},{"cell_type":"code","execution_count":77,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:36:37.197296Z","start_time":"2019-06-17T02:36:08.451184Z"},"id":"8wGvHTg-LtEb","outputId":"dcc31f90-b0f8-4f5d-c835-25be80638e70"},"outputs":[{"name":"stdout","output_type":"stream","text":["GB, Count Vectors    : 0.7970\n","\n","CPU times: total: 23.9 s\n","Wall time: 27.1 s\n"]}],"source":["%%time\n","# Gradient Boosting on Count Vectors\n","accuracy1 = train_model(GradientBoostingClassifier(), X_train_count, y_train, X_test_count)\n","print('GB, Count Vectors    : %.4f\\n' % accuracy1)"]},{"cell_type":"code","execution_count":78,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:36:52.852454Z","start_time":"2019-06-17T02:36:37.714920Z"},"id":"HJNwQf57LtEd","outputId":"686abbc5-8745-4dca-c2e2-b665b9d19901"},"outputs":[{"name":"stdout","output_type":"stream","text":["GB, WordLevel TF-IDF : 0.8075\n","\n","CPU times: total: 9.77 s\n","Wall time: 10.6 s\n"]}],"source":["%%time\n","# Gradient Boosting on Word Level TF IDF Vectors\n","accuracy2 = train_model(GradientBoostingClassifier(), X_train_tfidf, y_train, X_test_tfidf)\n","print('GB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"]},{"cell_type":"code","execution_count":79,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:37:02.608379Z","start_time":"2019-06-17T02:36:53.252355Z"},"id":"iyPqLMgkLtEe","outputId":"b7c87e04-f724-4ccf-e02e-28beb7e40654"},"outputs":[{"name":"stdout","output_type":"stream","text":["GB, N-Gram Vectors   : 0.7455\n","\n","CPU times: total: 5.73 s\n","Wall time: 5.94 s\n"]}],"source":["%%time\n","# Gradient Boosting on Ngram Level TF IDF Vectors\n","accuracy3 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n","print('GB, N-Gram Vectors   : %.4f\\n' % accuracy3)"]},{"cell_type":"code","execution_count":80,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:39:14.314194Z","start_time":"2019-06-17T02:37:03.039224Z"},"id":"1KYdyatTLtEg","outputId":"aa4f303e-76ff-47e6-a04f-7e311e1848ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["GB, CharLevel Vectors: 0.8140\n","\n","CPU times: total: 1min 30s\n","Wall time: 1min 32s\n"]}],"source":["%%time\n","# Gradient Boosting on Character Level TF IDF Vectors\n","accuracy4 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n","print('GB, CharLevel Vectors: %.4f\\n' % accuracy4)"]},{"cell_type":"code","execution_count":81,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:39:14.683222Z","start_time":"2019-06-17T02:39:14.675213Z"},"id":"AC0hWO59LtEj","scrolled":true},"outputs":[],"source":["results.loc['Gradient Boosting'] = {\n","    'Count Vectors': accuracy1,\n","    'WordLevel TF-IDF': accuracy2,\n","    'N-Gram Vectors': accuracy3,\n","    'CharLevel Vectors': accuracy4}"]},{"cell_type":"code","execution_count":82,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T02:39:15.024279Z","start_time":"2019-06-17T02:39:15.010319Z"},"id":"b9fY4J7XLtEk","outputId":"ea4a8ddf-5b37-4cb9-da79-1920e1967b03"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Count Vectors</th>\n","      <th>WordLevel TF-IDF</th>\n","      <th>N-Gram Vectors</th>\n","      <th>CharLevel Vectors</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Naïve Bayes</th>\n","      <td>0.8465</td>\n","      <td>0.8550</td>\n","      <td>0.8370</td>\n","      <td>0.8060</td>\n","    </tr>\n","    <tr>\n","      <th>Logistic Regression</th>\n","      <td>0.8730</td>\n","      <td>0.8780</td>\n","      <td>0.8435</td>\n","      <td>0.8445</td>\n","    </tr>\n","    <tr>\n","      <th>Support Vector Machine</th>\n","      <td>0.8465</td>\n","      <td>0.8710</td>\n","      <td>0.8330</td>\n","      <td>0.8545</td>\n","    </tr>\n","    <tr>\n","      <th>Random Forest</th>\n","      <td>0.8300</td>\n","      <td>0.8385</td>\n","      <td>0.7945</td>\n","      <td>0.7960</td>\n","    </tr>\n","    <tr>\n","      <th>Gradient Boosting</th>\n","      <td>0.7970</td>\n","      <td>0.8075</td>\n","      <td>0.7455</td>\n","      <td>0.8140</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                        Count Vectors  WordLevel TF-IDF  N-Gram Vectors  \\\n","Naïve Bayes                    0.8465            0.8550          0.8370   \n","Logistic Regression            0.8730            0.8780          0.8435   \n","Support Vector Machine         0.8465            0.8710          0.8330   \n","Random Forest                  0.8300            0.8385          0.7945   \n","Gradient Boosting              0.7970            0.8075          0.7455   \n","\n","                        CharLevel Vectors  \n","Naïve Bayes                        0.8060  \n","Logistic Regression                0.8445  \n","Support Vector Machine             0.8545  \n","Random Forest                      0.7960  \n","Gradient Boosting                  0.8140  "]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["results"]},{"cell_type":"markdown","metadata":{"id":"RERADKgNFq9T"},"source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","> > > > > > > > > © 2022 Institute of Data\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n"]}],"metadata":{"colab":{"collapsed_sections":["_Pck1cuvLtDH","mQCAUFWYLtDb","OXwLriDpLtDq","-2oNfajULtD4","q1wYto68LtEE","gLGxWK0yLtEO"],"name":"IOD_Lab-8_5.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}
